{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:07:41.455918Z",
     "start_time": "2020-03-14T14:07:41.451929Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./git/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are trying to predict some value $y_i$ using some variable $x_i$, and you use a linear model to try to do so.\n",
    "\n",
    "Taking the basic equation of a straight line, you suppose that the gradient and y-axis cutting point are $\\alpha$ and $\\beta$, giving you:\n",
    "\n",
    "$$y_i = \\beta x_i + \\alpha + \\varepsilon_i$$\n",
    "\n",
    "where $\\varepsilon_i$ is some small error term representing the fact that there are other factors which this model does not take into account.\n",
    "\n",
    "Assuming we've already figured out $\\alpha$ and $\\beta$, then making prediction is trivially easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:00:33.853123Z",
     "start_time": "2020-03-14T14:00:33.849073Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(alpha:float, \n",
    "            beta:float, \n",
    "            x_i:float) -> float:\n",
    "    return beta*x_i + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do you pick your `alpha` and `beta` values? Since we already know some actual values of y_i, you can start by computing the error for each pair of values i.e. how wrong are the predictions for each pair of alpha and beta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:04:49.370088Z",
     "start_time": "2020-03-14T14:04:49.365101Z"
    }
   },
   "outputs": [],
   "source": [
    "def error(alpha: float, \n",
    "          beta: float,\n",
    "          x_i: float,\n",
    "          y_i:float) -> float:\n",
    "    \"\"\"\n",
    "    The error from predicting beta * x_i + alpha\n",
    "    when the actual value is y_i.\n",
    "    \"\"\"\n",
    "    return predict(alpha,beta,x_i) - y_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gives us the error value of each point, but what about some value of error for the whole dataset?\n",
    "\n",
    "You can't just add up the individual errors of each data point because + and - errors will cancel out. That's why you use the *mean squared error*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:09:34.854364Z",
     "start_time": "2020-03-14T14:09:34.848381Z"
    }
   },
   "outputs": [],
   "source": [
    "from scratch.linear_algebra import Vector\n",
    "\n",
    "def sum_of_sqerrors(alpha: float,\n",
    "                    beta: float,\n",
    "                    x: Vector,\n",
    "                    y: Vector) -> float:\n",
    "    return sum(error(alpha,beta,x_i,y_i)**2 \n",
    "               for x_i, y_i in zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *least squares solution* is the pair of alpha-beta values that minimise the sum of squared errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:16:28.748389Z",
     "start_time": "2020-03-14T14:16:28.741410Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from scratch.linear_algebra import Vector\n",
    "from scratch.statistics import correlation, standard_deviation, mean\n",
    "\n",
    "def least_squares_fit(x: Vector, \n",
    "                      y: Vector) -> Tuple[float,float]:\n",
    "    \"\"\"\n",
    "    Given two vectors x and y,\n",
    "    find the least-squares values of alpha and beta\n",
    "    \"\"\"\n",
    "    beta = correlation(x,y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta*mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the algebra and calculus behind this equation, lets think about the intuition behind it.\n",
    "\n",
    "Our chosen alpha value says that when we see the average value of the independent variable `x` we predict the average value of variable `y`.\n",
    "\n",
    "Our chosen beta value means that when the input value increases by `standard_deviation(x)`, the prediction increases by `correlation(x,y) * standard_deviation(y)`.\n",
    "\n",
    "In the case where `x` and `y` are perfectly correlated, one standard deviation increase in `x` results in one standard deviation increase in `y`, our prediction.\n",
    "\n",
    "When they are perfectly anti-correlated, one standard deviation increase in `x` results in a one standard deviation *decrease* in `y`, our prediction.\n",
    "\n",
    "When the correlation is 0, beta is 0, which means that changes in `x` don't affect the prediction at all.\n",
    "\n",
    "Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:27:05.088882Z",
     "start_time": "2020-03-14T14:27:05.081889Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(-100,100,10)]\n",
    "y = [3*i -5 for i in x]\n",
    "\n",
    "assert least_squares_fit(x,y) == (-5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:29:36.015436Z",
     "start_time": "2020-03-14T14:29:36.009389Z"
    }
   },
   "outputs": [],
   "source": [
    "from scratch.statistics import num_friends_good, daily_minutes_good\n",
    "\n",
    "alpha, beta = least_squares_fit(num_friends_good,daily_minutes_good)\n",
    "assert 22.9 < alpha < 23.0\n",
    "assert 0.9 < beta < 0.905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:31:15.016077Z",
     "start_time": "2020-03-14T14:31:15.011048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.94755241346903\n",
      "0.903865945605865\n"
     ]
    }
   ],
   "source": [
    "print(alpha)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we would expect: \n",
    "- a user with n friends to spend 22.95 + (n x 0.903) minutes on the site each day, in this particular example;\n",
    "- a user with no friends (x) to still spend about 23 minutes a day on the site\n",
    "- for each additional friend, we expect a user to spend almost a minute more on the site each day.\n",
    "\n",
    "We can test and understand this intuition by looking a chart of the scattered data points with the straight line regression superimposed, but that's not very scientific.\n",
    "\n",
    "The scientific way to do it is to use the 'r-squared value' or the 'coefficient of determination', which measures the fraction of the total variation in the dependent variable that is captured by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:38:49.184148Z",
     "start_time": "2020-03-14T14:38:49.177166Z"
    }
   },
   "outputs": [],
   "source": [
    "from scratch.statistics import de_mean\n",
    "\n",
    "def total_sum_of_squares(y: Vector) -> float:\n",
    "    \"\"\"\n",
    "    the total squared variation of y_i's from their mean\n",
    "    \"\"\"\n",
    "    return sum(v**2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha: float, \n",
    "              beta:float, \n",
    "              x: Vector, \n",
    "              y: Vector) -> float:\n",
    "    return 1.0 - (sum_of_sqerrors(alpha,beta,x,y)/total_sum_of_squares(y))\n",
    "\n",
    "rsq = r_squared(alpha,\n",
    "                beta,\n",
    "                num_friends_good,\n",
    "                daily_minutes_good)\n",
    "assert 0.328 < rsq < 0.330"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the r-squared value, the better our model fits the data. \n",
    "\n",
    "In this case, the value is 0.329 which tells us that the model is only kind of meh at fitting the data, and that clearly there are other factors at play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write `theta = [alpha, beta]` we can also solve this using gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:52:38.022516Z",
     "start_time": "2020-03-14T14:52:27.752730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 13196.619: 100%|██████████████████████████████████████████████████████████| 10000/10000 [00:10<00:00, 976.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tqdm\n",
    "from scratch.gradient_descent import gradient_step\n",
    "\n",
    "num_epochs = 10000\n",
    "random.seed(0)\n",
    "guess = [random.random(), random.random()]\n",
    "learning_rate = 0.00001\n",
    "\n",
    "with tqdm.trange(num_epochs) as t:\n",
    "    for _ in t:\n",
    "        alpha, beta = guess\n",
    "        grad_a = sum(2*error(alpha,beta,x_i,y_i) \n",
    "                     for x_i, y_i in zip(num_friends_good,\n",
    "                                         daily_minutes_good))\n",
    "        grad_b = sum(2*error(alpha,beta,x_i,y_i) * x_i \n",
    "                     for x_i,y_i in zip(num_friends_good,\n",
    "                                        daily_minutes_good))\n",
    "        \n",
    "        loss = sum_of_sqerrors(alpha,\n",
    "                               beta,\n",
    "                               num_friends_good,\n",
    "                               daily_minutes_good)\n",
    "        \n",
    "        t.set_description(f\"loss: {loss:.3f}\")\n",
    "        \n",
    "        guess = gradient_step(guess,\n",
    "                              [grad_a, grad_b],\n",
    "                              -learning_rate)\n",
    "        \n",
    "alpha,beta = guess\n",
    "assert 22.9 < alpha < 23.0\n",
    "assert 0.9 < beta < 0.905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T14:53:16.516203Z",
     "start_time": "2020-03-14T14:53:16.509225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.947552155340915\n",
      "0.9038659662765034\n"
     ]
    }
   ],
   "source": [
    "print(alpha)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why choose least squares as our method of assessing errors?\n",
    "\n",
    "One argument is from *maximum likelihood estimation*. Imagine we have a sample of data $v_1,...,v_n$ that comes from a distribution that depends on some unknown parameter $\\theta$:\n",
    "\n",
    "$$p(v_1,...v_n|\\theta)$$\n",
    "\n",
    "If we didn't know $\\theta$ we could turn around and think of this quantity as the likelihood of  $\\theta$ given the sample:\n",
    "\n",
    "$$L(\\theta|v_1,...,v_n)$$\n",
    "\n",
    "Under with approach, the most likely $\\theta$ is the value that maximises this likelihood function - that is, the value that makes the observed data the most probable. Given a continuous distribution where we have a probability density function rather than a mass function, we can do the same thing.\n",
    "\n",
    "One assumption that's made about simple linear regression models is that the regression errors are normally distributed with mean 0 and some (known) standard deviation $\\sigma$. If thats the case, then the likelihood based on seeing a pair `(x_i, y_i)` is:\n",
    "\n",
    "$$L(\\alpha,\\beta|x_i,y_i,\\sigma) = (1/\\sqrt(2\\pi\\sigma))exp(-(y_i - \\alpha - \\beta x_i)^2/(2\\sigma^2))$$\n",
    "\n",
    "The likelihood based on the entire dataset is the product of the individual likelihoods, which is largest precisely when `alpha` and `beta` are chosen to minimise the sum of squared errors.\n",
    "\n",
    "Therefore, in this case, under these assumptions, minimising the sum of squared errors is equivalent to maximising the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
