{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-28T16:23:58.190188Z",
     "start_time": "2020-03-28T16:23:58.184704Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A spam filter: principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple spam filter might approach classifying spam emails in this way:\n",
    "\n",
    "\"what is the probabiility that an email is spam given that it contains the word Bitcoin?\"\n",
    "\n",
    "Bayes Theorem holds that the probability that the message is sam conditional on containing the word bitcoin is:\n",
    "\n",
    "$$P(S|B)=P(B|S)P(S) / (P(B|S)P(S) + P(B|S')P(S'))$$\n",
    "\n",
    "If you have a big collection of emails that you know or are sure are spam, you can easily estimate the probabilities $P(B|S)$, $P(S|S')$.\n",
    "\n",
    "A more sophisticated one might scale this up and instead say:\n",
    "\n",
    "\"Given: \n",
    "- this list of words $w_1, ... ,w_n$;\n",
    "- the event $X_i$ that an email contains word $w_i$; \n",
    "- some estimate $P(X_i|S)$ for the probability that a spam message contains the *i*th word; and\n",
    "- some estimate $P(X_i|~S)$ for the probability that a non-spam message contains the *i*th word,\n",
    "\n",
    "what is the probability that a message is spam?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption behind this is that the presence or absence of words $w_i$ to $w_n$ in a given message are independent of one another save for whether or not a message is spam.\n",
    "\n",
    "This is a **huge** assumption, and hence this technique is called _Naive_ Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that our 'vocabulary' of spam words was only 'Bitcoin' and 'Rolex', and that half of all spam messages are abouting earning Bitcoin while the other half is about cheap Rolexes.\n",
    "\n",
    "In this case the Naive Bayes estimate that a spam message contains both bitcoin and rolex is:\n",
    "\n",
    "$$P(X_1 = 1, X_2 = 1|S) = P(X_1 = 1|S) P(X_2 = 1|S) = 0.5 * 0.5 = 0.25$$\n",
    "\n",
    "Since we assumed away the knowledge that bitcoin and rolex actually never occur together.\n",
    "\n",
    "Despite this, however, this technique often performs pretty well and is actually used in spam filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Bayes Theorem again, we can calculate the probability that a message is spam using the equation \n",
    "\n",
    "$$P(S|X = x) = P(X = x|S) / [P(X = x|S + P(X = x|S')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NB assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word.\n",
    "\n",
    "In practice, however, you don't want to multiply together to many probabilities because computers can run into difficulties multiplying many floating point values <1 together (called *underflow*).\n",
    "\n",
    "Therefore, easy workaround, we use logarithms! Remember that $log(ab) = log(a) + log(b)$ and that $exp(log(x)) = x$, we usually compute $p_q * ... * p_n$ as the equivalent but floating-point-friendlier:\n",
    "\n",
    "$$exp(log(p_1) + ... + log(p_n))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then left with figuring out a way to come up with estimates for $P(X_i|S)$ and $P(X_i|S')$ the probabilities that a spam message or nonspam message contains the word $w_i$.\n",
    "\n",
    "This causes problems though - imagine that our training set contains the word 'data' only in nonspam messages. Our NB classifier would always assign spam probability 0 to any message that contains 'data on free bitcoin and authentic rolex watches'. \n",
    "\n",
    "To get around this we use some kind of smoothing.\n",
    "\n",
    "We choose a *pseudocount* $k$ and estimate the probability of seeing the *i*th word in a spam message as:\n",
    "\n",
    "$$P(X_i|S) = (k + number of spams containing  w_i) / (2k + number of spams)$$\n",
    "\n",
    "And similarly with $P(X_i|S')$. That is, when computing the spam probabilities for the *i*th word, we assume we also saw k additional nonspams containing the word and k additional nonspams not containing the word.\n",
    "\n",
    "For example, if 'data' occurs in 0/98 spam messages, and if k = 1, we estimate $P(data|S)$ as 1/100 = 0.01, which allows our classifier to still assign some nonzero spam probability to messages that contain the word data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A spam filter: implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in play, let's build a classifier up.\n",
    "\n",
    "First up, lets create a function to *tokenise* messages into distinct words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:24:37.845867Z",
     "start_time": "2020-03-14T11:24:37.839885Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower() # make lower case\n",
    "    all_words = re.findall(\"[a-z0-9']+\",text) # extract the words\n",
    "    return set(all_words) # remove the duplicates\n",
    "\n",
    "assert tokenize(\"Data science is science\") == {'data','science','is'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define a type for our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:24:37.853846Z",
     "start_time": "2020-03-14T11:24:37.847862Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our classifier needs to keep track of tokens, counts, and labels from the training data, we'll make it a class. Per convention, we'll call nonspam emails 'ham' emails.\n",
    "\n",
    "The constructor will take one parameter, the pseudocount to use when computing probabilities. It also initialises an empty set of tokens, counters to track how often each token is seen in spam messages and ham messages, and counts of how many spam and ham messages it was trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:24:37.870840Z",
     "start_time": "2020-03-14T11:24:37.856837Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self,k:float = 0.5) -> None:\n",
    "        self.k = k # smoothing factor\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str,int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str,int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "    \n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "            \n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "    \n",
    "    def _probabilities(self,token:str) -> Tuple[float,float]:\n",
    "        \"\"\"returns P(token | spam) and P(token | ham)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "        \n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2*self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2*self.k)\n",
    "        \n",
    "        return p_token_spam, p_token_ham\n",
    "    \n",
    "    def predict(self,text:str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "        \n",
    "        # Iterate through each word in our vocab\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "            \n",
    "            # If token in message, \n",
    "            # add the log probability of seeing it\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "                \n",
    "            # Otherwise add the log prob of _not_ seeing it\n",
    "            # which is (1 - prob of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A spam filter: testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it works by writing some unit tests for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:24:37.879790Z",
     "start_time": "2020-03-14T11:24:37.872795Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [Message(\"spam rules\", is_spam = True),\n",
    "            Message(\"ham rules\", is_spam = False),\n",
    "            Message(\"Hello ham\", is_spam = False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)\n",
    "\n",
    "assert model.tokens == {'spam','ham','rules','hello'}\n",
    "assert model.spam_messages == 1\n",
    "assert model.ham_messages == 2\n",
    "assert model.token_spam_counts == {'spam':1,'rules':1}\n",
    "assert model.token_ham_counts == {'ham':2,'rules':1,'hello':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a prediction and compare it to a hand-calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:24:37.890747Z",
     "start_time": "2020-03-14T11:24:37.881788Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'hello spam'\n",
    "\n",
    "probs_if_spam = [\n",
    "    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present)\n",
    "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
    "    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present)\n",
    "]\n",
    "\n",
    "probs_if_ham = [\n",
    "    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present)\n",
    "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
    "    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present)\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "\n",
    "assert model.predict(text) == p_if_spam / (p_if_ham + p_if_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it works with training wheels on. Let's do it on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A spam filter: using our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:26:45.303671Z",
     "start_time": "2020-03-14T11:26:45.299583Z"
    }
   },
   "outputs": [],
   "source": [
    "# from io import BytesIO\n",
    "# import requests\n",
    "# import tarfile\n",
    "\n",
    "# base_url = \"https://spamassassin.apache.org/old/publiccorpus\"\n",
    "# files = [\"20021010_easy_ham.tar.bz2\",\n",
    "#          \"20021010_hard_ham.tar.bz2\",\n",
    "#          \"20021010_spam.tar.bz2\"]\n",
    "\n",
    "# output_dir = 'spam_data'\n",
    "\n",
    "# for file in files:\n",
    "#     content = requests.get(f\"{base_url}/{file}\").content\n",
    "#     fin = BytesIO(content)\n",
    "#     with tarfile.open(fileobj=fin,mode='r:bz2') as tf:\n",
    "#         tf.extractall(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple we'll only look at the subject lines of the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:26:58.662934Z",
     "start_time": "2020-03-14T11:26:49.039516Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob, re\n",
    "\n",
    "path = 'spam_data/*/*'\n",
    "\n",
    "data: List[Message] = []\n",
    "    \n",
    "for file in glob.glob(path):\n",
    "    is_spam = \"ham\" not in file\n",
    "    \n",
    "    with open(file,errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject,is_spam))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split into train-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:27:38.135899Z",
     "start_time": "2020-03-14T11:27:38.106978Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from scratch.machine_learning import split_data\n",
    "random.seed(0)\n",
    "\n",
    "train_messages, test_messages = split_data(data,0.75)\n",
    "\n",
    "model = NaiveBayesClassifier()\n",
    "model.train(train_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some predictions and see how the model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:29:27.244382Z",
     "start_time": "2020-03-14T11:29:24.105774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 670, (True, True): 86, (True, False): 40, (False, True): 29})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "predictions = [(message, model.predict(message.text))\n",
    "               for message in test_messages]\n",
    "\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5) \n",
    "                           for message, spam_probability \n",
    "                           in predictions)\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives 84 true positives (spam classified as “spam”), 25 false positives (ham classified as “spam”), 703 true negatives (ham classified as “ham”), and 44 false negatives (spam classified as “ham”). This means our precision is 84 / (84 + 25) = 77%, and our recall is 84 / (84 + 44) = 65%, which are not bad numbers for such a simple model. (Presumably we’d do better if we looked at more than the subject lines.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:33:27.035308Z",
     "start_time": "2020-03-14T11:33:27.024295Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_spam_given_token(token:str, model: NaiveBayesClassifier) -> float:\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "\n",
    "words = sorted(model.tokens,key=lambda t: p_spam_given_token(t,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:34:11.522608Z",
     "start_time": "2020-03-14T11:34:11.516595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spammiest words ['zzzz', 'attn', '95', 'clearance', 'per', 'money', 'sale', 'rates', 'systemworks', 'adv']\n"
     ]
    }
   ],
   "source": [
    "print(\"spammiest words\", words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T11:34:23.272178Z",
     "start_time": "2020-03-14T11:34:23.264199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hammiest words ['spambayes', 'users', 'razor', 'zzzzteana', 'sadev', 'apt', 'perl', 'ouch', 'spamassassin', 'selling']\n"
     ]
    }
   ],
   "source": [
    "print(\"hammiest words\", words[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
